{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "파일[test.jpg] 전송종료. 전송량 [1031297]\n",
      "파일[test.jpg] 전송종료. 전송량 [1031215]\n"
     ]
    },
    {
     "ename": "ConnectionRefusedError",
     "evalue": "[WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mConnectionRefusedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-2c0c20ec10d1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;32mwhile\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m     \u001b[0mfilename\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'test.jpg'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m     \u001b[0mgetFileFromServer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     37\u001b[0m     \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m11\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[1;31m#\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-2c0c20ec10d1>\u001b[0m in \u001b[0;36mgetFileFromServer\u001b[1;34m(filename)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0msocket\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msocket\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msocket\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAF_INET\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msocket\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSOCK_STREAM\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m         \u001b[0msock\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mHOST\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mPORT\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m         \u001b[0msock\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msendall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mConnectionRefusedError\u001b[0m: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다"
     ]
    }
   ],
   "source": [
    "import socket\n",
    "import time\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import re\n",
    "from datetime import datetime\n",
    "\n",
    "HOST = '192.168.0.10'\n",
    "PORT = 8585\n",
    "TRAINING_FILE = '/Users/wjdghd/training_data_class5_40/training_file.txt'\n",
    "VALIDATION_FILE = '/Users/wjdghd/training_data_class5_40/validate_file.txt'\n",
    "\n",
    "flags = tf.app.flags\n",
    "FLAGS = flags.FLAGS\n",
    "FLAGS.image_size = 96\n",
    "FLAGS.image_color = 3\n",
    "FLAGS.maxpool_filter_size = 2\n",
    "FLAGS.num_classes = 5\n",
    "FLAGS.batch_size = 100\n",
    "FLAGS.learning_rate = 0.0001\n",
    "FLAGS.log_dir = '/Users/wjdghd/'\n",
    "\n",
    "sock=socket.socket(socket.AF_INET,socket.SOCK_STREAM)\n",
    "###\n",
    "def get_input_queue(csv_file_name, num_epochs = None):\n",
    "    train_images = []\n",
    "    train_labels = []\n",
    "    for line in open(csv_file_name, 'r'):\n",
    "        cols = re.split(',|\\n',line)\n",
    "        train_images.append(cols[0])\n",
    "        train_labels.append(int(cols[2]))\n",
    "    #과적합을 방지하기 위해 shuffle=True옵션을 추가한다.    \n",
    "    input_queue = tf.train.slice_in3put_producer([train_images, train_labels], num_epochs, shuffle = True)\n",
    "    return input_queue\n",
    "###\n",
    "def read_ data(input_queue):\n",
    "    image_file = input_queue[0]\n",
    "    label = input_queue[1]\n",
    "    #jpeg이미지를 읽어서 데이터 객체로 저장한다.\n",
    "    image = tf.image.decode_jpeg(tf.read_file(image_file), channels = FLAGS.image_color)\n",
    "    \n",
    "    return image, label, image_file\n",
    "###\n",
    "def read_data_batch(csv_file_name,batch_size = FLAGS.batch_size):\n",
    "    input_queue = get_input_queue(csv_file_name)\n",
    "    image, label, file_name = read_data(input_queue)\n",
    "    image = tf.reshape(image,[FLAGS.image_size,FLAGS.image_size,FLAGS.image_color])\n",
    "    \n",
    "    image = tf.image.random_flip_left_right(image)\n",
    "    image = tf.image.random_brightness(image,max_delta=0.5)\n",
    "    image = tf.image.random_contrast(image, lower=0.2, upper = 2.0)\n",
    "    image = tf.image.random_hue(image,max_delta=0.08)\n",
    "    image = tf.image.random_saturation(image,lower=0.2,upper=2.0)\n",
    "    \n",
    "    batch_image,batch_label,batch_file = tf.train.batch([image,label,file_name],batch_size = batch_size)\n",
    "    batch_file = tf.reshape(batch_file,[batch_size,1])\n",
    "    \n",
    "    batch_label_on_hot=tf.one_hot(tf.to_int64(batch_label), FLAGS.num_classes, on_value=1.0,off_value=0.0)\n",
    "    return batch_image,batch_label_on_hot,batch_file\n",
    "###\n",
    "def conv1(input_data):\n",
    "    FLAGS.conv1_filter_size = 3\n",
    "    FLAGS.conv1_layer_size = 16\n",
    "    FLAGS.stride1 = 1\n",
    "    \n",
    "    with tf.name_scope('conv_1'):\n",
    "        W_conv1 = tf.Variable(tf.truncated_normal([FLAGS.conv1_filter_size,FLAGS.conv1_filter_size,FLAGS.conv0_layer_size,FLAGS.conv1_layer_size],stddev=0.1))\n",
    "        b1 = tf.Variable(tf.truncated_normal([FLAGS.conv1_layer_size],stddev=0.1))\n",
    "        h_conv1 = tf.nn.conv2d(input_data,W_conv1,strides=[1,1,1,1],padding = 'SAME')\n",
    "        h_conv1_relu = tf.nn.relu(tf.add(h_conv1,b1))\n",
    "        h_conv1_maxpool = tf.nn.max_pool(h_conv1_relu,ksize = [1,2,2,1],stride=[1,2,2,1],padding='SAME')\n",
    "    return h_conv1_maxpool\n",
    "###\n",
    "def conv2(input_data):\n",
    "    FLAGS.conv2_filter_size = 3\n",
    "    FLAGS.conv2_layer_size = 32\n",
    "    FLAGS.stride2 = 1\n",
    "    with tf.name_scope('conv_2'):\n",
    "        W_conv2 = tf.Variable(tf.truncated_mormal([FLAGS.conv2_filter_size,FLAGS.conv2_filter_size, FLAGS.conv1_layer_size,FLAGS.conv2_layer_size],stddev=0.1))\n",
    "        b2 = tf.Variable(tf.truncated_normal([FLAGS.conv2_layer_size],stddev=0.1))\n",
    "        h_conv2=tf.nn.conv2d(input_data,W_conv2,strides=[1,1,1,1],padding='SAME')\n",
    "        h_conv2_relu = tf.nn.relu(tf.add(h_conv2,b2))\n",
    "        h_conv2_maxpool = tf.nn.max_pool(h_conv2_relu,ksize=[1,2,2,1],strides=[1,2,2,1],padding='SAME')\n",
    "    return h_conv2_maxpool\n",
    "###\n",
    "def conv3(input_data):\n",
    "    FLAGS.conv3_filter_size = 3\n",
    "    FLAGS.conv3_layer_size = 64\n",
    "    FLAGS.strides3 = 1\n",
    "    with tf.name_scope('conv_3'):\n",
    "        W_conv3 = tf.Variable(tf.truncated_normal([FLAGS.conv3_filter_size,FLAGS.conv3_filter_size,FLAGS.conv2_layer_size,FLAGS.conv3_layer_size],stddev=0.1))\n",
    "        b3 = tf.Variable(tf.truncated_normal([FLAGS.conv3_layer_size],stddev=0.1))\n",
    "        h_conv3 = tf.nn.conv2d(input_data,W_conv3,strides=[1,1,1,1],padding='SAME')\n",
    "        h_conv3_relu = tf.nn.relu(tf.add(h_conv3,b3))\n",
    "        h_conv3_maxpool = tf.nn.max_pool(h_conv3_relu,ksize=[1,2,2,1],strides=[1,2,2,1],padding='SAME')\n",
    "    return h_conv3_maxpool\n",
    "###\n",
    "def conv4(input_data):\n",
    "    FLAGS.conv4_filter_size = 3\n",
    "    FLAGS.conv4_layer_size = 128\n",
    "    FLAGS.strides4 = 1\n",
    "    with tf.name_scope('conv_4'):\n",
    "        W_conv4 = tf.Variable(tf.truncated_normal([FLAGS.conv4_filter_size,FLAGS.conv4_filter_size,FLAGS.conv3_layer_size,FLAGS.conv4_layer_size],stddev=0.1))\n",
    "        b4 = tf.Variable(tf.truncated_normal([FLAGS.conv4_layer_size],stddev=0.1))\n",
    "        h_conv4 = tf.nn.conv2d(input_data,W_conv4,strides=[1,1,1,1],padding='SAME')\n",
    "        h_conv4_relu = tf.nn.relu(tf.add(h_conv4,b4))\n",
    "        h_conv4_maxpool = tf.nn.max_pool(h_conv4_relu,ksize=[1,2,2,1],strides=[1,2,2,1],padding='SAME')\n",
    "    return h_conv4_maxpool\n",
    "###Fully connected layer1\n",
    "def fc1(input_data):\n",
    "    input_layer_size = 6*6*FLAGS.conv4_lauer_size\n",
    "    FLAGS.fc1_lauer_size = 512\n",
    "    \n",
    "    with tf.name_scope('fc_1'):\n",
    "        input_data_reshape = tf.reshape(input_data,[-1,input_layer_size])\n",
    "        W_fc1 = tf.Variable(tf.truncated_normal([input_layer_size,FLAGS.fc1_layer_size],stddev=0.1))\n",
    "        b_fc1 = tf.add(tf.matmul(input_data_reshape,W_fc1),b_fc1)# h_fc1 = input_data*W_fc1+b_fc1\n",
    "        h_fc1_relu = tf.nn.relu(h_fc1)\n",
    "        \n",
    "    return h_fc1_relu\n",
    "###Fullyconnected layer2\n",
    "def fc2(input_data):\n",
    "    FLAGS.fc2_layer_size = 256\n",
    "    with tf.name_scope('fc_2'):\n",
    "        W_fc2 = tf.Variable(tf.truncated_normal([FLAGS.fc1_layer_size,FLAGS.fc2_layer_size],stddev=0.1))\n",
    "        b_fc2 = tf.Variable(tf.truncated_normal([FLAGS.fc2_layer_size],stddev=0.1))\n",
    "        h_fc2 = tf.add(tf.matmul(input_data,W_fc2),b_fc2)\n",
    "        \n",
    "    return h_fc2_relu\n",
    "###final layer\n",
    "def final_out(input_data):\n",
    "    with tf.name_scope('final_out'):\n",
    "        W_fo = tf.Variable(tf.truncated_normal([FLAGS.fc2_layer_size,FLAGS.num_classes],stddev=0.1))\n",
    "        b_fo = tf.Variable(tf.truncated_normal([FLAGS.num_classes],stddev=0.1))\n",
    "        h_fo = tf.add(tf.matmul(input_data,W_fo),b_fo)\n",
    "        \n",
    "    return h_fo\n",
    "## build cnn_graph\n",
    "def build_model(images,keep_prob):\n",
    "    # define CNN network graph\n",
    "    # output shape will be (*,48,48,16)\n",
    "    r_cnn1 = conv1(images) # convolutional layer 1\n",
    "    print (\"shape after cnn1 \",r_cnn1.get_shape())\n",
    "    \n",
    "    # output shape will be (*,24,24,32)\n",
    "    r_cnn2 = conv2(r_cnn1) # convolutional layer 2\n",
    "    print (\"shape after cnn2 :\",r_cnn2.get_shape() )\n",
    "    \n",
    "    # output shape will be (*,12,12,64)\n",
    "    r_cnn3 = conv3(r_cnn2) # convolutional layer 3\n",
    "    print (\"shape after cnn3 :\",r_cnn3.get_shape() )\n",
    "\n",
    "    # output shape will be (*,6,6,128)\n",
    "    r_cnn4 = conv4(r_cnn3) # convolutional layer 4\n",
    "    print (\"shape after cnn4 :\",r_cnn4.get_shape() )\n",
    "    \n",
    "    # fully connected layer 1\n",
    "    r_fc1 = fc1(r_cnn4)\n",
    "    print (\"shape after fc1 :\",r_fc1.get_shape() )\n",
    "\n",
    "    # fully connected layer2\n",
    "    r_fc2 = fc2(r_fc1)\n",
    "    print (\"shape after fc2 :\",r_fc2.get_shape() )\n",
    "    \n",
    "    ## drop out\n",
    "    # 참고 http://stackoverflow.com/questions/34597316/why-input-is-scaled-in-tf-nn-dropout-in-tensorflow\n",
    "    # 트레이닝시에는 keep_prob < 1.0 , Test 시에는 1.0으로 한다. \n",
    "    r_dropout = tf.nn.dropout(r_fc2,keep_prob)\n",
    "    print (\"shape after dropout :\",r_dropout.get_shape() ) \n",
    "    \n",
    "    # final layer\n",
    "    r_out = final_out(r_dropout)\n",
    "    print (\"shape after final layer :\",r_out.get_shape() )\n",
    "\n",
    "    return r_out \n",
    "\n",
    "def main(argv=None):\n",
    "    \n",
    "    # define placeholders for image data & label for traning dataset\n",
    "    \n",
    "    images = tf.placeholder(tf.float32,[None,FLAGS.image_size,FLAGS.image_size,FLAGS.image_color])\n",
    "    labels = tf.placeholder(tf.int32,[None,FLAGS.num_classes])\n",
    "    image_batch,label_batch,file_batch = read_data_batch(TRAINING_FILE) \n",
    "\n",
    "    keep_prob = tf.placeholder(tf.float32) # dropout ratio\n",
    "    prediction = build_model(images,keep_prob)\n",
    "    # define loss function\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=prediction,labels=labels))\n",
    "\n",
    "    tf.summary.scalar('loss',loss)\n",
    "\n",
    "    #define optimizer\n",
    "    optimizer = tf.train.AdamOptimizer(FLAGS.learning_rate)\n",
    "    train = optimizer.minimize(loss)\n",
    "\n",
    "\n",
    "    # for validation\n",
    "    #with tf.name_scope(\"prediction\"):\n",
    " \n",
    "    validate_image_batch,validate_label_batch,validate_file_batch = read_data_batch(VALIDATION_FILE)\n",
    "    label_max = tf.argmax(labels,1)\n",
    "    pre_max = tf.argmax(prediction,1)\n",
    "    correct_pred = tf.equal(tf.argmax(prediction,1),tf.argmax(labels,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_pred,tf.float32))\n",
    "            \n",
    "    tf.summary.scalar('accuracy',accuracy)\n",
    "        \n",
    "    \n",
    "    startTime = datetime.now()\n",
    "    \n",
    "    #build the summary tensor based on the tF collection of Summaries\n",
    "    summary = tf.summary.merge_all()\n",
    "    \n",
    "    with tf.Session(config=tf.ConfigProto(allow_soft_placement=True, log_device_placement=True)) as sess:\n",
    "        saver = tf.train.Saver() # create saver to store training model into file\n",
    "        summary_writer = tf.summary.FileWriter(FLAGS.log_dir,sess.graph)\n",
    "        \n",
    "        init_op = tf.global_variables_initializer() # use this for tensorflow 0.12rc0\n",
    "        coord = tf.train.Coordinator()\n",
    "        threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n",
    "        sess.run(init_op)\n",
    "        \n",
    "        for i in range(10000):\n",
    "            images_,labels_ = sess.run([image_batch,label_batch])\n",
    "            #sess.run(train_step,feed_dict={images:images_,labels:labels_,keep_prob:0.8})\n",
    "            sess.run(train,feed_dict={images:images_,labels:labels_,keep_prob:0.7})\n",
    "            \n",
    "            if i % 10 == 0:\n",
    "                now = datetime.now()-startTime\n",
    "                print('## time:',now,' steps:',i)         \n",
    "                \n",
    "                # print out training status\n",
    "                rt = sess.run([label_max,pre_max,loss,accuracy],feed_dict={images:images_ \n",
    "                                                          , labels:labels_\n",
    "                                                          , keep_prob:1.0})\n",
    "                print ('Prediction loss:',rt[2],' accuracy:',rt[3])\n",
    "                # validation steps\n",
    "                validate_images_,validate_labels_ = sess.run([validate_image_batch,validate_label_batch])\n",
    "                rv = sess.run([label_max,pre_max,loss,accuracy],feed_dict={images:validate_images_ \n",
    "                                                          , labels:validate_labels_\n",
    "                                                          , keep_prob:1.0})\n",
    "                print ('Validation loss:',rv[2],' accuracy:',rv[3])\n",
    "                if(rv[3] > 0.9):\n",
    "                    break\n",
    "                # validation accuracy\n",
    "                summary_str = sess.run(summary,feed_dict={images:validate_images_ \n",
    "                                                          , labels:validate_labels_\n",
    "                                                          , keep_prob:1.0})\n",
    "                summary_writer.add_summary(summary_str,i)\n",
    "                summary_writer.flush()\n",
    "        \n",
    "        saver.save(sess, 'face_recog') # save session\n",
    "        coord.request_stop()\n",
    "        coord.join(threads)\n",
    "        print('finish')\n",
    "    \n",
    "###라즈베리파이 통신함수\n",
    "def getFileFromServer(filename):\n",
    "    data_transferred = 0\n",
    " \n",
    "    with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as sock:\n",
    "        sock.connect((HOST,PORT))\n",
    "        sock.sendall(filename.encode())\n",
    " \n",
    "        data = sock.recv(1024)\n",
    "        if not data:\n",
    "            print('파일[%s]: 서버에 존재하지 않거나 전송중 오류발생' %filename)\n",
    "            return\n",
    " \n",
    "        with open('download/' + filename, 'wb') as f:\n",
    "            try:\n",
    "                while  data:\n",
    "                    f.write(data)\n",
    "                    data_transferred += len(data)\n",
    "                    data = sock.recv(1024)\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "            finally:\n",
    "                sock.close()\n",
    " \n",
    "    print('파일[%s] 전송종료. 전송량 [%d]' %(filename, data_transferred))\n",
    "\n",
    "while 1:\n",
    "    filename = 'test.jpg'\n",
    "    getFileFromServer(filename)\n",
    "    time.sleep(11)\n",
    "#\n",
    "#   전송받은 사진으로 머신러닝 실행\n",
    "main()\n",
    "#   실행 결과를 통해 알게된 작물의 환경값을 아래 소켓에 전송\n",
    "#    \n",
    "#    sock.connect(('192.168.0.10',8585))\n",
    "#    sock.send(\"머신러닝으로 알아낸 작물의 환경값\".encode())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
